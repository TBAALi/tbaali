# MachineLearning

[TOC]

## 序言

```shell
本文档为 Machine Leraning 学习中的一些总结记录以及机器学习算法的实现。



Autor: TBAALi
Emil: jiaxx903@foxmail.com
```



## 模型评估与选择



## 线性模型



## K-近邻算法（KNN）

### 工作原理

* 存在一个样本数据集合，也称为训练样本，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据 (最近邻) 的分类标签。一般来说，我们只选择样本数据集中前 k 个最相似的数据，这就是 k-近邻算法中 k 的出处，通常 k 是不大于 20 的整数。最后选择 k 个最相似数据中出现次数最多的分类，最为新数据的分类。

### 优缺点

* 优点：精度高，对异常值不敏感，无数据输入假定。
* 缺点：计算复杂度高，空间复杂度高。
* 适用数据范围：数值型和标称型。

### 归一化

* 处理不同取值范围的特征值，通常将其归一化，将取值范围定到 0 - 1 或 -1 - 1. 下面的公式可以将取值范围定到 0 - 1

  ```c++
  newValue = (oldValue - min) / (max - min)    
  ```

  其中 min 、 max 分别是数据集中的最小特征和最大特征值。



## 决策树

### 工作原理

* 决策树是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个节点代表一种分类结果。

* 监督学习就是给出一堆样本，每个样本都有一组属性和一个分类结果，也就是分类结果已知，那么通过学习这些样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。

* 节点的分裂：一般当一个节点所代表的属性无法给出判断时，则选择将这一节点分成两个节点 (如不是二叉树的情况会分成 n 个子节点)。

* 阈值的确定：选择适当地阈值使得分类错误率最小。

* 比较常见的决策树有 ID3，C4.5 和 CRT，CRT 的分类效果一般优于其他决策树。

* 由熵增原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类的结果越好。

* 熵定义为信息的期望值
  $$
  l(x_i)=-log_2p(x_i)
  $$
  计算所有类别素有可能值包含的信息期望值
  $$
  H=-\sum_{i=1}^{n}p(x_i)log_2p(x_i)
  $$
  熵用于表示系统混乱程度，系统越混乱 （熵越大）数据集越不纯。决策树的生成便是使用某特征对数据集进行划分，从而使得划分后各数据子集的纯度比划分钱的数据纯度高，这种划分券后纯度 (熵) 的差值称为信息增益，又成3信息。显然，信息增益越大，该特征越具有决策能力。公式如下：
  $$
  Gain(D, A)=H(D)-H(D|A)
  $$
  我们将利用信息增益递归的选择特征划分数据集的方法称为 ID3.

* 信息增益率与 C4.5：使用信息增益的缺点就是特征偏向于具有大量取值的特征，就是说如果一个特征所取不同值的个数越多，则该特征就越可能用来做分裂点。最极端的情况就是该特征每个结果都对应一个不同的特征取值，那么求得信息熵为 0，信息增益最大。为弥补 ID3 缺陷，在此基础上，又提出了一种改进的特征选择准则，公式为：
  $$
  GainRatio(A) = Gain(A) / H(A)
  $$
  其中，Gain(A) 代表使用特征 A 划分数据集产生的信息增益，H(A) 为 A 特征的信息熵。

* 基尼指数与 CRAT ：基尼指数又称基尼不纯度，也可以用来度量数据划分的纯度，定义为：
  $$
  Gini=\sum_{i=1}^{m}p_i(1-p_i)=1- \sum_{i=1}^{m}p_i^2
  $$
  即基尼指数 = 选中样本被分对的概率 × 选中样本被分错的概率 = 选中样本被分对的概率  × （1 - 选中样本被反对的概率）= 1 - 样本中被分对的概率^2.

### 优缺点

* 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
* 缺点：可能会产生过度匹配的问题。
* 适用数据类型：数值和标称型。

## 神经网络



## 支持向量机



## Logistic 回归

### 工作原理

* 我们想要的函数应该是能接受所有的输入然后预测出类别。Sigmoid 函数具体计算公式如下:
  $$
  σ(z) = {\frac{1}{1+e^{-z}}}
  $$
  Sigmoid 函数的输入记为 z，由下面的公式得出：
  $$
  z = w_0x_0 + x_1x_1 + w_2x_2 + ... + w_nx_n
  $$
  如果采用向量的写法，上述公式可以写为：
  $$
  z = W^TX
  $$
  

### 优缺点

* 优点：计算代价不高，易于理解和实现。
* 缺点：容易欠拟合，分类精度可能不高。
* 适用数据类型：数值型和标称型。



## 贝叶斯分类器

### 工作原理

* 贝叶斯公式：
  $$
  P(B|A) = P(A|B)P(B) / P(A)
  $$
  换个表达形式，如下
  $$
  P(类别|特征) = P(特征|类别)P(类别) / P(特征)
  $$
  朴素贝叶斯算法是假设各个特征之间相互独立。

* 条件概率：
  $$
  p(c|x) = p(x|c)p(c) / p(x)
  $$
  

### 优缺点

* 优点：在数据极少的情况下任然于晓，可以处理多类别问题。算法逻辑简单，易于实现。
* 缺点：对于输入数据的准备方式较为敏感。理论上朴素贝叶斯与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是优于朴素贝叶斯模型假设属性之间想胡独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间关联性比较大时，分类效果不好。
* 适用数据类型：标称型数据。

## 集成学习



## 聚类



## 降维与度量学习



## 特征选择与稀疏学习



## 计算学习理论



## 半监督学习



## 概率图模型



## 规则学习



## 强化学习

## 

## 后记

1. 数据挖掘十大算法